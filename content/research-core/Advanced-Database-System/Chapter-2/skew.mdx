---
title: Skew
description:
parent: Parallel Databases
grand_parent : Advanced Database System
order: 6
completed: true
---

Handling skew in database partitioning is essential for balancing the workload and avoiding bottlenecks. Skew happens when data is unevenly distributed across disks, causing some disks to have more tuples than others. This imbalance affects performance and reduces parallelism.

## Types of Skew

### 1. Attribute-Value Skew
This type of skew happens when certain attribute values are significantly more common than others. When a partitioning scheme, such as hash-partitioning or range-partitioning, relies on specific attribute values to assign data to different disks, the overrepresentation of a particular value can lead to overloading a single disk.

#### Example of Attribute-Value Skew

Imagine a table Orders partitioned by a Country attribute to store orders from different countries. Let’s say that a large proportion of the orders come from one country, such as the United States. If the partitioning scheme does not account for this uneven distribution, it might place all orders from the United States on a single disk, leading to an overload on that disk while others remain underutilized.

#### For example:
* **Hash-partitioning:** Suppose we hash the Country attribute and use the hash result to assign data to disks. If most values hash to the same disk due to common attribute values, one disk might become overloaded with orders.
* **Range-partitioning:** If the partitioning vector divides countries alphabetically (e.g., Disk 0: A–F, Disk 1: G–M, Disk 2: N–Z), countries with high order volumes, such as the United States or Germany, may overload specific disks.

### 2. Partition Skew
Partition skew happens when the boundaries or ranges in a partitioning scheme inadvertently assign too much data to certain partitions. This type of skew is most commonly associated with range-partitioning, where the partitioning vector determines how data is divided across disks. Poorly chosen partition boundaries can lead to an unbalanced load distribution.

#### Example of Partition Skew
Consider a Sales table partitioned by Amount with the following partition vector:
* Disk 0: Sales amounts from $0 to $500
* Disk 1: Sales amounts from $500 to $1000
* Disk 2: Sales amounts above $1000
If most sales are under $500, Disk 0 could be overloaded with data, while Disks 1 and 2 have significantly fewer tuples. This situation limits parallelism since one disk does most of the work, negating the benefits of a distributed system.

---

## Comparision Table

| Aspect                | **Attribute-Value Skew**                                      | **Partition Skew**                                      |
|-----------------------|----------------------------------------------------------------|---------------------------------------------------------|
| **Definition**        | Imbalance due to overly common attribute values                | Imbalance due to poorly defined partition boundaries    |
| **Partitioning Type** | Common in **hash** and **range partitioning**                  | Common in **range partitioning**                        |
| **Cause**             | High frequency of specific values, e.g., one country with many orders | Unequal data ranges, e.g., most sales below a certain value |
| **Impact**            | Overloads disks/processors for certain values                  | Overloads specific partitions, reducing parallelism     |
| **Example**           | Orders from the US overload a single disk                     | Sales below $500 overload one disk                      |

---

## Techniques for Handling Skew

### 1. Balanced Partitioning Vector (For Range Partitioning)
* A balanced partitioning vector is a range-partitioning technique that aims to evenly distribute data across partitions based on the sorted values of an attribute. It involves dividing the sorted dataset into equal segments to ensure that each partition holds approximately the same number of records. This technique works well if the data is uniformly distributed but may not be sufficient in cases of attribute skew, where certain values (e.g., many records with a sale amount of $50) overload a specific range and, consequently, a specific partition.
* This technique still may not handle cases with high duplication in attribute values.
**Example:**
If we have 1 million sales records and use a partitioning vector that assigns equal data ranges (e.g., Disk 0: $0–$250, Disk 1: $250–$500, etc.), a skew in the data (like many $50 sales) will still overload Disk 0. Histogram-based partitioning can provide a better balance by accounting for the distribution of values within each range.

### 2. Histogram-Based Partitioning

Histogram-Based Partitioning is an advanced data partitioning method designed to address issues of data skew more effectively than simple range partitioning. It leverages the actual distribution of values in a dataset to create partitions that are balanced in terms of the number of records, even when the data has a skewed distribution.

#### Why Histogram-Based Partitioning?
When using traditional range partitioning, we often assume that data values are uniformly distributed. However, in real-world datasets, certain values or ranges may appear far more frequently than others (data skew). Histogram-based partitioning tackles this by:
* **Analyzing the frequency distribution** of values for the partitioning attribute.
* **Creating partitions** with boundaries that reflect this distribution, ensuring each partition has a similar number of records rather than equal range sizes.

#### How Histogram-Based Partitioning Works
* **Build a Histogram of the Data Distribution:** A histogram shows the frequency of data values across different ranges. By plotting the number of records within each range, we can understand where the data is dense (i.e., where many records fall within a small range) and where it is sparse (i.e., where few records exist within a large range).

* **Set Partition Boundaries Based on Frequency:** Using the histogram, we divide the dataset into partitions such that each partition contains a similar number of records. This approach dynamically adapts to the data distribution, assigning narrower ranges to dense data areas and broader ranges to sparse areas.

* **Assign Partitions to Disks/Nodes:** Once partitions are defined, they can be assigned to different disks or nodes. Each partition now represents a balanced subset of records, regardless of how skewed the original data was.

#### Example of Histogram-Based Partitioning
Consider a scenario with 1 million sales records in a database, with each record having a sale amount attribute. Let's assume we want to partition these records across 4 disks to balance the load.

* **Step 1:** Analyze Data Distribution
    A simple histogram analysis reveals the following distribution of sale amounts:
    * $0–$50: 600,000 records
    * $50–$200: 300,000 records
    * $200–$500: 80,000 records
    * $500+: 20,000 records
    Here, most records fall in the $0–$50 range, while higher ranges (like $500+) contain far fewer records.

* **Step 2:** Define Partition Boundaries Based on Distribution
Using histogram-based partitioning, we can create partitions that reflect this distribution instead of using fixed ranges. Since we have 4 disks, we aim to divide the data into four partitions, each containing about 250,000 records:
    * Partition 1 (Disk 0): $0–$20 (250,000 records)
        We choose a smaller range because the density is very high in the lower range of $0–$50.
    * Partition 2 (Disk 1): $20–$50 (250,000 records)
        Another dense range, so we keep it small to balance the number of records.
    * Partition 3 (Disk 2): $50–$200 (300,000 records)
        This range is less dense than the previous one, so we can allow a wider range.
    * Partition 4 (Disk 3): $200+ (200,000 records)
        Since the data above $200 is sparse, we group all records above this value into a single partition.

* **Step 3:** Partition Allocation
Now that each partition has approximately 250,000 records, we can assign them to the four disks. This allocation ensures that each disk has a similar load, despite the data skew.

#### Advantages of Histogram-Based Partitioning
* Balances Load Even with Skewed Data: By adapting partition boundaries to data density, this method balances partitions more effectively than traditional range partitioning.
* Efficient Use of Resources: Since each partition holds roughly the same number of records, resource utilization is more efficient across all nodes or disks.
* Scalable: This method can be applied to large datasets and adjusted as data distribution changes over time.

---

## Virtual Processor Partitioning
Virtual Processor Partitioning is a technique commonly used in distributed systems, virtualization, and multi-threaded environments to optimize how computational resources (specifically processor cores) are allocated to different tasks or workloads. This approach is especially relevant in systems where physical processors (cores) are shared across multiple virtual processors or threads, such as in cloud environments, virtual machines, or multi-core processors.

### Key Concepts in Virtual Processor Partitioning
**Virtual Processors:** Virtual processors are logical representations of physical processors (or cores) that a system’s operating system or hypervisor allocates to different workloads. In virtualized environments, these virtual processors are distributed among different virtual machines (VMs) or containers, each receiving a portion of the host system's physical processing power.

**Partitioning of Processor Resources:** Partitioning involves dividing available processing resources into smaller units and assigning them to various virtual processors. This is done to:
    * Ensure that each task, virtual machine, or container has access to adequate processing power.
    * Improve system performance by balancing workloads and minimizing idle processor time.
    * Allow for isolated execution of tasks, improving security and stability by preventing one task from interfering with another.

**Hypervisor Role:** In virtualized environments, a hypervisor (e.g., VMware ESXi, Microsoft Hyper-V, KVM) manages the allocation of physical processor resources to virtual processors. It determines how and when virtual processors access physical CPUs based on factors like workload demand, system load, and configured resource limits.

### Steps for Handling Skew with Virtual Processor Partitioning

* **Create a Large Number of Partitions:**
    * To handle skew effectively, we create a large number of virtual partitions—typically 10 to 20 times the number of physical processors.
    * **For example,** if a system has 4 physical processors, we would create around 40 to 80 virtual partitions.
    * By having many small partitions, we can spread out the skewed data across multiple partitions, rather than overloading any single partition or processor.

* **Assign Virtual Partitions to Physical Processors:**
    * After creating multiple virtual partitions, they are mapped to the physical processors using one of two methods:

    * **a. Round-Robin Assignment:**
        * Partitions are assigned in a rotating, sequential order across all processors.
        * For example, if there are 4 processors and 40 virtual partitions, each processor receives 10 partitions in a cyclic pattern.
        * This approach ensures an equal number of partitions per processor, regardless of the load or processing cost.

    * **b. Cost-Based Assignment:**
        * Partitions are assigned based on an estimated cost of processing each virtual partition, such as the expected processing time or size of data within each partition.
        * Higher-cost partitions are assigned to different processors to avoid overloading any single processor.
        * This approach takes into account the complexity or volume of data in each partition, balancing the processing time across processors more precisely.

* **Basic Idea Behind This Technique:**
    * By creating more partitions than processors, we effectively spread out the skewed data across a larger number of partitions.
    * Even if some virtual partitions contain skewed data, the impact of the skew is minimized because the skewed data is distributed over multiple processors.
    * For example, if there is a high concentration of records with a specific attribute value, that data may be split across several virtual partitions, and each of these partitions is handled by different processors.
